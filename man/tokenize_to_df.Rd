% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/tokenize.R
\name{tokenize_to_df}
\alias{tokenize_to_df}
\title{Create a data.frame of tokens}
\usage{
tokenize_to_df(
  x,
  text_field = "text",
  docid_field = "doc_id",
  into = dict_features(),
  col_select = seq_along(into),
  instance = rebuild_tokenizer(),
  ...
)
}
\arguments{
\item{x}{A data.frame like object or a character vector to be tokenized.}

\item{text_field}{Column name where to get texts to be tokenized.}

\item{docid_field}{Column name where to get identifiers of texts.}

\item{into}{Column names of features.}

\item{col_select}{Character or integer vector of column names
that kept in the return value. When passed as \code{NULL},
returns comma-separated features as is.}

\item{instance}{A binding to the instance of \verb{<sudachipy.tokenizer.Tokenizer>}.
If you already have a tokenizer instance,
you can improve performance by providing a predefined instance.}

\item{...}{Currently not used.}
}
\value{
A tibble.
}
\description{
Create a data.frame of tokens
}
\examples{
\dontrun{
tokenize_to_df(
  "Tokyo, Japan",
  into = dict_features("en"),
  col_select = c("pos1", "pos2")
)
}
}
